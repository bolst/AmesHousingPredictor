{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352eafe6",
   "metadata": {},
   "source": [
    "# Phase 2: Model Training and Experimentation\n",
    "\n",
    "This notebook will focus on training and optimizing our model. Our objectives include\n",
    "1. Experiment tracking with MLflow\n",
    "2. Cross-validation\n",
    "3. Hyperparameter optimization\n",
    "4. Model evaluation and selection\n",
    "5. Model versioning and registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e963f08b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T05:03:52.711998Z",
     "iopub.status.busy": "2025-11-05T05:03:52.711754Z",
     "iopub.status.idle": "2025-11-05T05:03:53.495768Z",
     "shell.execute_reply": "2025-11-05T05:03:53.495471Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nic/git/AmesHousingPredictor/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# ignore this: required for run_notebooks.sh\n",
    "%pip install mlflow optuna --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1e21f93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T05:03:53.496890Z",
     "iopub.status.busy": "2025-11-05T05:03:53.496809Z",
     "iopub.status.idle": "2025-11-05T05:03:55.239983Z",
     "shell.execute_reply": "2025-11-05T05:03:55.239709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (1460, 80)\n",
      "Test shape: (1459, 79)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Loading data...\")\n",
    "import ames_notebooks\n",
    "from app.data_ingestion.read_data import DataReader\n",
    "\n",
    "reader = DataReader()\n",
    "train_data, test_data = reader.load_train_test()\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d2f21cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get preprocessing pipeline\n",
    "from app.pipelines.preprocessing import get_fitted_pipelines\n",
    "feature_preprocessor, target_transformer = get_fitted_pipelines(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e50cb6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T05:03:55.241123Z",
     "iopub.status.busy": "2025-11-05T05:03:55.241050Z",
     "iopub.status.idle": "2025-11-05T05:03:55.257211Z",
     "shell.execute_reply": "2025-11-05T05:03:55.257005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing pipeline...\n",
      "X_train shape: (1168, 241)\n",
      "X_val shape: (292, 241)\n"
     ]
    }
   ],
   "source": [
    "X = train_data.drop('SalePrice', axis=1)\n",
    "y = train_data['SalePrice']\n",
    "\n",
    "# apply pipelines/transformations\n",
    "X_processed = feature_preprocessor.transform(X)\n",
    "y_processed = target_transformer.transform(y)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "# use _val to prevent confusion between the test dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying preprocessing pipeline...\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329ebd4",
   "metadata": {},
   "source": [
    "## Baseline Model Development\n",
    "\n",
    "Let's start with a simple baseline model using XGBoost with default parameters. This will give us a reference point for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd21bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.33887\n",
      "[99]\tvalidation_0-rmse:0.14973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1115: UserWarning: [13:48:45] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025/11/12 13:48:47 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1124: UserWarning: [13:48:47] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1511: Unknown file format: `xgb`. Using UBJSON (`ubj`) as a guess.\n",
      "  self.get_booster().load_model(fname)\n",
      "2025/11/12 13:48:48 INFO mlflow.models.model: Found the following environment variables used during model inference: [API_KEY, STRIPE_API_KEY]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "train_rmse: 0.0078\n",
      "train_mae: 0.0054\n",
      "train_r2: 0.9996\n",
      "\n",
      "Validation Metrics:\n",
      "test_rmse: 0.1497\n",
      "test_mae: 0.1038\n",
      "test_r2: 0.8799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'xgboost-baseline' already exists. Creating a new version of this model...\n",
      "Created version '7' of model 'xgboost-baseline'.\n"
     ]
    }
   ],
   "source": [
    "from app.training.trainer import ModelTrainer\n",
    "\n",
    "model = xgb.XGBRegressor(random_state=42, n_jobs=-1)\n",
    "\n",
    "trainer = ModelTrainer(tracking_uri=\"./mlruns\")\n",
    "baseline_model, train_metrics, val_metrics = trainer.train(model, \"xgboost-baseline\", X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4956c",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Optuna\n",
    "\n",
    "Now that we have a baseline model, let's use Optuna to find better hyperparameters for our XGBoost model. We'll define an objective function that Optuna will optimize using cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bf2030",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T05:03:57.450533Z",
     "iopub.status.busy": "2025-11-05T05:03:57.450445Z",
     "iopub.status.idle": "2025-11-05T05:04:23.603663Z",
     "shell.execute_reply": "2025-11-05T05:04:23.603294Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-12 13:48:48,362] A new study created in memory with name: no-name-38721353-5d72-4fa0-86c9-2d7d98888e9d\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "486e88017cad4e06885fbddc789e6e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value:  0.1232095428370713\n",
      "  Params: \n",
      "    max_depth: 4\n",
      "    learning_rate: 0.01819413361560552\n",
      "    n_estimators: 721\n",
      "    min_child_weight: 6\n",
      "    subsample: 0.6304701750629337\n",
      "    colsample_bytree: 0.67748179186674\n",
      "    reg_alpha: 6.117201107029435e-08\n",
      "    reg_lambda: 0.4848445852613158\n"
     ]
    }
   ],
   "source": [
    "# objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # get model with suggested parameters\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    # cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        cv=5, \n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # mean negative RMSE (Optuna minimizes objective)\n",
    "    return -cv_scores.mean()\n",
    "\n",
    "# Create and run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9087202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.42725\n",
      "[100]\tvalidation_0-rmse:0.18090\n",
      "[200]\tvalidation_0-rmse:0.14626\n",
      "[300]\tvalidation_0-rmse:0.13866\n",
      "[400]\tvalidation_0-rmse:0.13570\n",
      "[500]\tvalidation_0-rmse:0.13487\n",
      "[600]\tvalidation_0-rmse:0.13482\n",
      "[700]\tvalidation_0-rmse:0.13502\n",
      "[720]\tvalidation_0-rmse:0.13504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1115: UserWarning: [13:49:26] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025/11/12 13:49:29 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1124: UserWarning: [13:49:29] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1511: Unknown file format: `xgb`. Using UBJSON (`ubj`) as a guess.\n",
      "  self.get_booster().load_model(fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "train_rmse: 0.0652\n",
      "train_mae: 0.0469\n",
      "train_r2: 0.9722\n",
      "\n",
      "Validation Metrics:\n",
      "test_rmse: 0.1350\n",
      "test_mae: 0.0881\n",
      "test_r2: 0.9023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'xgboost-optimized' already exists. Creating a new version of this model...\n",
      "Created version '2' of model 'xgboost-optimized'.\n"
     ]
    }
   ],
   "source": [
    "best_params = study.best_params\n",
    "best_params['random_state'] = 42\n",
    "model = xgb.XGBRegressor(**best_params)\n",
    "\n",
    "optimized_model, train_metrics, val_metrics = trainer.train(model, \"xgboost-optimized\", X_train, y_train, X_val, y_val)\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nValidation Metrics:\")\n",
    "for metric, value in val_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa358f3",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare the performance of our baseline and optimized models to see the improvement from hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "049b2257",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T05:04:26.140932Z",
     "iopub.status.busy": "2025-11-05T05:04:26.140856Z",
     "iopub.status.idle": "2025-11-05T05:04:26.146427Z",
     "shell.execute_reply": "2025-11-05T05:04:26.146236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Metrics:\n",
      "rmse: 0.1497\n",
      "mae: 0.1038\n",
      "r2: 0.8799\n",
      "\n",
      "Optimized Model Metrics:\n",
      "rmse: 0.1350\n",
      "mae: 0.0881\n",
      "r2: 0.9023\n",
      "\n",
      "RMSE Improvement: 9.81%\n"
     ]
    }
   ],
   "source": [
    "from app.training.trainer import evaluate_model\n",
    "\n",
    "# compare models on validation set\n",
    "baseline_metrics = evaluate_model(baseline_model, X_val, y_val)\n",
    "optimized_metrics = evaluate_model(optimized_model, X_val, y_val)\n",
    "\n",
    "print(\"Baseline Model Metrics:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nOptimized Model Metrics:\")\n",
    "for metric, value in optimized_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "improvement = (baseline_metrics['rmse'] - optimized_metrics['rmse']) / baseline_metrics['rmse'] * 100\n",
    "print(f\"\\nRMSE Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8a43acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full pipeline to ../models/full_pipeline.joblib\n",
      "Example prediction (SalePrice): [123347.48]\n"
     ]
    }
   ],
   "source": [
    "# Compose the preprocessor and trained model into a single sklearn-compatible object\n",
    "# and use TransformedTargetRegressor so the target transformer is applied automatically.\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "# Build an sklearn Pipeline that first applies the feature preprocessor then the model.\n",
    "# Note: feature_preprocessor and model are expected to be already fitted objects.\n",
    "skl_pipeline = Pipeline([\n",
    "    (\"preprocessor\", feature_preprocessor),\n",
    "    (\"regressor\", model),\n",
    "])\n",
    "\n",
    "# Wrap with TransformedTargetRegressor so predictions are returned on the original scale.\n",
    "full_pipeline = TransformedTargetRegressor(regressor=skl_pipeline, transformer=target_transformer)\n",
    "full_pipeline.fit(X, y)\n",
    "\n",
    "# Save the composed object (sklearn pipeline or fallback wrapper)\n",
    "models_path = Path(\"../models\")\n",
    "models_path.mkdir(parents=True, exist_ok=True)\n",
    "joblib.dump(full_pipeline, models_path / \"full_pipeline.joblib\")\n",
    "print(f\"Saved full pipeline to {models_path / 'full_pipeline.joblib'}\")\n",
    "\n",
    "# Example: predict on a single row from the test set (keeps original columns)\n",
    "example_row = test_data.iloc[[0]]  # DataFrame with one row\n",
    "pred = full_pipeline.predict(example_row)\n",
    "print(\"Example prediction (SalePrice):\", pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmesHousingPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
