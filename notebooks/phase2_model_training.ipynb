{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "352eafe6",
   "metadata": {},
   "source": [
    "# Phase 2: Model Training and Experimentation\n",
    "\n",
    "This notebook will focus on training and optimizing our model. Our objectives include\n",
    "1. Experiment tracking with MLflow\n",
    "2. Cross-validation\n",
    "3. Hyperparameter optimization\n",
    "4. Model evaluation and selection\n",
    "5. Model versioning and registration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e21f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Train shape: (1460, 81)\n",
      "Test shape: (1459, 80)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import mlflow\n",
    "import joblib\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from pathlib import Path\n",
    "\n",
    "# import custom transformers\n",
    "import os, sys\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "from models.custom_transformers import FeatureEngineer, LogTransformer\n",
    "\n",
    "# Configure MLflow\n",
    "EXPERIMENT_NAME = \"ames-housing-price-prediction\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# Load the preprocessor\n",
    "# if these errors out, try running phase1_eda.ipynb first\n",
    "models_dir = \"../models\"\n",
    "feature_preprocessor = joblib.load(f'{models_dir}/feature_preprocessor.joblib')\n",
    "target_transformer = joblib.load(f'{models_dir}/target_transformer.joblib')\n",
    "\n",
    "print(\"Loading data...\")\n",
    "train_data = pd.read_csv('../data/house-prices-advanced-regression-techniques/train.csv')\n",
    "test_data = pd.read_csv('../data/house-prices-advanced-regression-techniques/test.csv')\n",
    "print(\"Train shape:\", train_data.shape)\n",
    "print(\"Test shape:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e50cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying preprocessing pipeline...\n",
      "X_train shape: (1168, 242)\n",
      "X_val shape: (292, 242)\n"
     ]
    }
   ],
   "source": [
    "X = train_data.drop('SalePrice', axis=1)\n",
    "y = train_data['SalePrice']\n",
    "\n",
    "# apply pipelines/transformations\n",
    "X_processed = feature_preprocessor.transform(X)\n",
    "y_processed = target_transformer.transform(y)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "# use _val to prevent confusion between the test dataset\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y_processed, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Applying preprocessing pipeline...\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329ebd4",
   "metadata": {},
   "source": [
    "## Baseline Model Development\n",
    "\n",
    "Let's start with a simple baseline model using XGBoost with default parameters. This will give us a reference point for further improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "091c1cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, prefix=''):\n",
    "    y_pred = model.predict(X)\n",
    "    rmse = np.sqrt(mean_squared_error(y, y_pred))\n",
    "    mae = mean_absolute_error(y, y_pred)\n",
    "    r2 = r2_score(y, y_pred)\n",
    "    \n",
    "    return {\n",
    "        f'{prefix}rmse': rmse,\n",
    "        f'{prefix}mae': mae,\n",
    "        f'{prefix}r2': r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31a91ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.33810\n",
      "[99]\tvalidation_0-rmse:0.15271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/10/30 16:23:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1115: UserWarning: [16:23:25] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025/10/30 16:23:27 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/10/30 16:23:27 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "train_rmse: 0.0076\n",
      "train_mae: 0.0055\n",
      "train_r2: 0.9996\n",
      "\n",
      "Validation Metrics:\n",
      "val_rmse: 0.1527\n",
      "val_mae: 0.1064\n",
      "val_r2: 0.8750\n",
      "\n",
      "Baseline model saved to ../models/baseline_xgboost.joblib\n"
     ]
    }
   ],
   "source": [
    "# for training baseline XGBoost model\n",
    "with mlflow.start_run(run_name=\"xgboost-baseline\"):\n",
    "    # create model with default parameters\n",
    "    model = xgb.XGBRegressor(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # train model\n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    train_metrics = evaluate_model(model, X_train, y_train, prefix='train_')\n",
    "    val_metrics = evaluate_model(model, X_val, y_val, prefix='val_')\n",
    "    \n",
    "    mlflow.log_params(model.get_params())\n",
    "    mlflow.log_metrics({**train_metrics, **val_metrics})\n",
    "    mlflow.xgboost.log_model(model, \"model\")\n",
    "    \n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    for metric, value in train_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Save the baseline model\n",
    "joblib.dump(model, f'{models_dir}/baseline_xgboost.joblib')\n",
    "print(f\"\\nBaseline model saved to {models_dir}/baseline_xgboost.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d4956c",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimization with Optuna\n",
    "\n",
    "Now that we have a baseline model, let's use Optuna to find better hyperparameters for our XGBoost model. We'll define an objective function that Optuna will optimize using cross-validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bf2030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-30 16:23:27,701] A new study created in memory with name: no-name-cafbf307-a3bc-403a-a313-acd705a8e278\n",
      "Best trial: 20. Best value: 0.124841: 100%|██████████| 50/50 [00:28<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  Value:  0.12484070609617366\n",
      "  Params: \n",
      "    max_depth: 4\n",
      "    learning_rate: 0.047588753062449786\n",
      "    n_estimators: 355\n",
      "    min_child_weight: 2\n",
      "    subsample: 0.84936860390881\n",
      "    colsample_bytree: 0.6186138763393498\n",
      "    reg_alpha: 1.8046864107985947e-05\n",
      "    reg_lambda: 1.4720283701765382e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# objective function for Optuna\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "    # get model with suggested parameters\n",
    "    model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "    # cross-validation\n",
    "    cv_scores = cross_val_score(\n",
    "        model, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        cv=5, \n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # mean negative RMSE (Optuna minimizes objective)\n",
    "    return -cv_scores.mean()\n",
    "\n",
    "# Create and run Optuna study\n",
    "study = optuna.create_study(direction='minimize')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969dbd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.41864\n",
      "[100]\tvalidation_0-rmse:0.14093\n",
      "[200]\tvalidation_0-rmse:0.13675\n",
      "[300]\tvalidation_0-rmse:0.13689\n",
      "[354]\tvalidation_0-rmse:0.13676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/xgboost/sklearn.py:1115: UserWarning: [16:23:56] WARNING: /Users/runner/work/xgboost/xgboost/src/c_api/c_api.cc:1575: Saving model in the UBJSON format as default.  You can use a file extension: `json` or `ubj` to choose between formats.\n",
      "  self.get_booster().save_model(fname)\n",
      "2025/10/30 16:23:58 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "\u001b[31m2025/10/30 16:23:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Metrics:\n",
      "train_rmse: 0.0459\n",
      "train_mae: 0.0343\n",
      "train_r2: 0.9862\n",
      "\n",
      "Validation Metrics:\n",
      "val_rmse: 0.1368\n",
      "val_mae: 0.0911\n",
      "val_r2: 0.8998\n",
      "\n",
      "Optimized model saved to ../models/optimized_xgboost.joblib\n"
     ]
    }
   ],
   "source": [
    "# for training the optimized model\n",
    "with mlflow.start_run(run_name=\"xgboost-optimized\"):\n",
    "    # get model with best parameters\n",
    "    best_params = study.best_params\n",
    "    best_params['random_state'] = 42\n",
    "    model = xgb.XGBRegressor(**best_params)\n",
    "    \n",
    "    model.fit(\n",
    "        X_train, \n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    train_metrics = evaluate_model(model, X_train, y_train, prefix='train_')\n",
    "    val_metrics = evaluate_model(model, X_val, y_val, prefix='val_')\n",
    "    \n",
    "    mlflow.log_params(model.get_params())\n",
    "    mlflow.log_metrics({**train_metrics, **val_metrics})\n",
    "    mlflow.xgboost.log_model(model, name=\"model\")\n",
    "    \n",
    "    print(\"\\nTraining Metrics:\")\n",
    "    for metric, value in train_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    print(\"\\nValidation Metrics:\")\n",
    "    for metric, value in val_metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# save optimized model\n",
    "joblib.dump(model, '../models/optimized_xgboost.joblib')\n",
    "print(\"\\nOptimized model saved to ../models/optimized_xgboost.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa358f3",
   "metadata": {},
   "source": [
    "## Model Comparison\n",
    "\n",
    "Let's compare the performance of our baseline and optimized models to see the improvement from hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "049b2257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Metrics:\n",
      "rmse: 0.1527\n",
      "mae: 0.1064\n",
      "r2: 0.8750\n",
      "\n",
      "Optimized Model Metrics:\n",
      "rmse: 0.1368\n",
      "mae: 0.0911\n",
      "r2: 0.8998\n",
      "\n",
      "RMSE Improvement: 10.45%\n"
     ]
    }
   ],
   "source": [
    "baseline_model = joblib.load('../models/baseline_xgboost.joblib')\n",
    "\n",
    "# compare models on validation set\n",
    "baseline_metrics = evaluate_model(baseline_model, X_val, y_val)\n",
    "optimized_metrics = evaluate_model(model, X_val, y_val)\n",
    "\n",
    "print(\"Baseline Model Metrics:\")\n",
    "for metric, value in baseline_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nOptimized Model Metrics:\")\n",
    "for metric, value in optimized_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "improvement = (baseline_metrics['rmse'] - optimized_metrics['rmse']) / baseline_metrics['rmse'] * 100\n",
    "print(f\"\\nRMSE Improvement: {improvement:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f703a",
   "metadata": {},
   "source": [
    "## Generate Predictions for Test Set\n",
    "\n",
    "Finally, we can use our optimized model to generate predictions for the test set. We'll transform the predictions back to the original scale and create a submission file which can be uploaded to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8c8b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved to ../submissions/xgboost_submission.csv\n",
      "\n",
      "First few predictions:\n",
      "     Id      SalePrice\n",
      "0  1461  122708.265625\n",
      "1  1462  162753.484375\n",
      "2  1463  183156.562500\n",
      "3  1464  195766.843750\n",
      "4  1465  186826.921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nic/git/AmesHousingPredictor/.venv/lib/python3.10/site-packages/sklearn/preprocessing/_encoders.py:246: UserWarning: Found unknown categories in columns [0, 4, 14, 15, 29, 30, 38] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# make predictions on test data\n",
    "X_test_processed = feature_preprocessor.transform(test_data)\n",
    "y_test_transformed = model.predict(X_test_processed)\n",
    "\n",
    "# Transform predictions back to original scale\n",
    "y_test = target_transformer.inverse_transform(y_test_transformed)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'Id': test_data.Id,\n",
    "    'SalePrice': y_test\n",
    "})\n",
    "\n",
    "# save submission file\n",
    "submission_path = '../submissions/xgboost_submission.csv'\n",
    "Path('../submissions').mkdir(exist_ok=True)\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"Submission file saved to {submission_path}\")\n",
    "\n",
    "print(\"\\nFirst few predictions:\")\n",
    "print(submission.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AmesHousingPredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
